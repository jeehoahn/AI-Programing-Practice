{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 색인 & 검색\n",
    "---\n",
    "## 코드 설명\n",
    "1. 온라인으로 사이트에서 12개의 논문 내용을 리스트 안에 넣기\n",
    "2. CharacterTextSplitter로 데이터 전처리\n",
    "3. ChromaDB로 내용을 저장\n",
    "4. Vocoder랑 관련된 논문 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain chromadb openai pypdf unstructured[pdf] gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.document_loaders import OnlinePDFLoader # Unstructured[pdf]가 필요한 이유\n",
    "\n",
    "data=[]\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1712.05884.pdf\") # 온라인 논문 가져오기\n",
    "data.append(str(loader.load()))                                  # str으로 하는 이유는 나중에\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2308.01546.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05664.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05922.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05191.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.04407.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2309.09088.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2309.15224.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.04716.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05897.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05688.pdf\")\n",
    "data.append(str(loader.load()))\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2310.05920.pdf\")\n",
    "data.append(str(loader.load()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "PDF_Text = text_splitter.create_documents(data)\n",
    "\n",
    "embeddings=GPT4AllEmbeddings()\n",
    "\n",
    "Things = Chroma.from_documents(PDF_Text, embedding=embeddings, persist_directory=\".\")\n",
    "Things.persist()\n",
    "\n",
    "query = \"Vocoder\"\n",
    "Result = Things.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[Document(page_content=\"Haoming Guo, Seth Z. Zhao, Jiachen Lian, Gopala Anumanchipalli, Gerald Friedland\\\\n\\\\nUniversity of California, Berkeley\\\\n\\\\n3 2 0 2\\\\n\\\\np e S 6 1\\\\n\\\\n]\\\\n\\\\nD S . s c [\\\\n\\\\n1 v 8 8 0 9 0 . 9 0 3 2 : v i X r a\\\\n\\\\nABSTRACT\\\\n\\\\nVocoder models have recently achieved substantial progress in generating authentic audio comparable to human quality while significantly reducing memory requirement and infer- ence time. However, these data-hungry generative models require large-scale audio data for learning good representa- tions. In this paper, we apply contrastive learning methods in training the vocoder to improve the perceptual quality of the vocoder without modifying its architecture or adding more data. We design an auxiliary task with mel-spectrogram con- trastive learning to enhance the utterance-level quality of the vocoder model under data-limited conditions. We also extend the task to include waveforms to improve the multi-modality comprehension of the model and address the discriminator overfitting problem. We optimize the additional task simulta- neously with GAN training objectives. Our result shows that the tasks improve model performance substantially in data- limited settings. Our analysis based on the result indicates that the proposed design successfully alleviates discriminator overfitting and produces audio of higher fidelity. Index Terms: GAN, self-supervised learning, vocoder\\\\n\\\\n1. INTRODUCTION\\\\n\\\\nGenerative Adversarial Networks (GANs) [2] have been widely used in vocoders and have achieved the state-of-the- art in the domain [3, 4, 5]. However, training GAN vocoders still meets two challenges, data insufficiency and discrimina- tor overfitting.\\\\n\\\\nspecifically on the application of contrastive learning. Al- though contrastive learning has been explored in the context of speech recognition [6], we are unaware of any previous efforts to apply this approach to Vocoder modeling. In this work, our aim is to leverage contrastive learning as an auxil- iary task to enhance the vocoding performance of GAN gen- erators under data-limited conditions.\\\\n\\\\nThe second challenge, discriminator overfitting, is also shown to be crucial, especially on small dataset [17, 18, 19], and the convergence of GAN also critically depends on the quality of discriminators [20]. Contrastive learning on the discriminator has been proved to alleviate this problem in image generation [21], and the method, in general, is also shown to increase model’s performance and robustness on vi- sion and language tasks [22, 23, 24, 25]. However, in speech synthesis, a naive approach of mel-spectrogram contrastive learning will only involve the generator, which encodes mel- spectrograms, but not the discriminator, which encodes the waveform. Therefore, we propose to extend the training to the discriminator by using a multi-modal contrastive task be- tween mel-spectrograms and waveforms.\\\\n\\\\nOur contributions can be summarized as the following.\\\\n\\\\n1. We propose a contrastive learning task with masked mel-spectrograms to improve the performance on lim- ited data.\\\\n\\\\n2. We design a novel contrastive learning task of matching mel-spectrogram to waveforms to regularize the dis- criminator and improve the perceptual quality of the generator.\\\\n\\\\nIn the realm of single-speaker speech synthesis, the lim- ited size of available datasets poses a significant challenge. To enhance the performance of vocoders operating under such constraints, we propose the use of unsupervised learning tech- niques to extract additional self-supervised signals for train- ing. Self-supervised learning (SSL) methods have demon- strated efficacy in a diverse array of speech domains, includ- ing representation learning [6, 7, 8, 9, 10], synthesis [11, 12, 13, 14], and multi-modality [15, 16]. Drawing on the excep- tional transfer learning capabilities of SSL, we seek to har- ness this power in the realm of Vocoder modeling, focusing\\\\n\\\\nThis paper is based on Haoming’s thesis [1] at University of California,\\\\n\\\\nBerkeley.\\\\n\\\\n3. We implement a framework for integrating contrastive learning into the GAN training pipeline.\\\\n\\\\n3. We implement a framework for integrating contrastive learning into the GAN training pipeline.\\\\n\\\\n4. We provide experimental results and in-depth analysis of the methods’ effectiveness compared to the baseline.\\\\n\\\\n2. METHODS\\\\n\\\\nIn this section, we first introduce the auxiliary contrastive task that we have designed for the GAN vocoder model. Sub- sequently, we explicate the details of how we modified the task to train both the generator and the discriminator of the\\\\n\\\\nvocoder model. Finally, we illustrate our proposed training framework, which synergizes the contrastive task with GAN objectives. It is worth noting that we have utilized the same model architecture as HiFi-GAN [4]. However, it is perti- nent to mention that our method can be applied to other GAN frameworks for vocoders as well.\\\\n\\\\n2.1. Mel-spectrogram Contrastive Learning\\\\n\\\\nIn our GAN model, the generator takes a mel-spectrogram as input and outputs a raw waveform through a stack of convolu- tional layers. We use a learnable feed-forward layer to project the features of the convolutional layers onto a latent space RD , where elements of similar semantics are close to each other through contrastive learning. For each anchor in a batch of N samples, we apply masking on randomly selected intervals in time and frequency to create a positive sample, while all other (N − 1) input samples and (N − 1) masked samples are used as negative samples. Together, the method results in 1 positive pair and 2(N − 1) negative pairs in the batch. We then adapt the InfoNCE loss [26] used in CLIP [27] for our loss function as follows:\\\\n\\\\nLcl = −\\\\n\\\\n1 N\\\\n\\\\nN (cid:88)\\\\n\\\\ni=1\\\\n\\\\n(cid:32)\\\\n\\\\nlog\\\\n\\\\n(cid:80)2N\\\\n\\\\nexp(τ vi · vk) j=1;i̸=j exp(τ vi · vj))\\\\n\\\\n(cid:33)\\\\n\\\\n(1)\\\\n\\\\nmatching. This task serves to train both the generator and the discriminators, promoting rich semantic representation and preventing overfitting of the discriminators to the real or fake classification. The method is illustrated in Fig. 2. For a batch of pairs of mel-spectrograms and waveforms, we assign the labels of the true pairs to be positive and those of the other pairs to be negative, resulting in N positive pairs and N (N − 1) negative pairs in a batch of N samples. We use the backbone of the generator to encode the mel-spectrogram and the backbone of the discriminator to encode the waveform. Similar to the method in section 2.1, we use two separate feed-forward layers to project each encoded feature to the same latent dimension RD. Then, we perform the modified loss function\\\\n\\\\nLcl = −\\\\n\\\\n1 N\\\\n\\\\nN (cid:88)\\\\n\\\\ni=1\\\\n\\\\n(cid:32)\\\\n\\\\nlog\\\\n\\\\n(cid:80)N\\\\n\\\\nexp(τ vi · wi) j=1;i̸=j exp(τ vi · wj))\\\\n\\\\n(cid:33)\\\\n\\\\nwhere wi ∈ RD is the latent embedding of the waveform cor- responding to the ith mel-spectrogram, vi ∈ RD is the latent embedding of the ith mel-spectrogram, and τ is a temperature parameter. HiFi-GAN contains multiple discriminators, so we calculate a contrastive loss between the mel-spectrogram embedding and each of the waveform embeddings and sum them up. For simplicity, we refer them as one discriminator in this paper unless otherwise mentioned.\\\\n\\\\n(2)\\\\n\\\\nwhere vk ∈ RD is the masked sample from vi ∈ RD and τ is a temperature parameter. This method is shown in Fig. 1.\\\\n\\\\nFig. 1. Illustration of Mel-spectrogram Contrastive Learning. The Mel Encoder is the backbone of the generator. This method only trains the generator in a GAN framework.\\\\n\\\\nFig. 2. Illustration of Mel-spectrogram & Waveform Con- trastive Learning. The Mel Encoder is the backbone of the generator, and the Wave Encoder is the backbone of the dis- criminator. Therefore, this method trains both the generator and discriminator.\\\\n\\\\n2.3. Multi-tasking Framework\\\\n\\\\n2.2. Mel-spectrogram Waveform Contrastive Learning\\\\n\\\\nIn addition to training solely the generator, we propose a novel task that involves contrastive spectrogram-waveform\\\\n\\\\nTo integrate contrastive learning with GAN tasks, we adopt a multi-tasking framework that makes auxiliary tasks a joint optimization objective with original learning goals [28]. As illustrated in Fig. 3, we create additional heads for the training\\\\n\\\\nReal/Fake\\\\n\\\\nDiscriminator\\\\n\\\\nDiscriminator\\\\n\\\\nReal/FakeReal Waveform\\\\n\\\\nGenerator\\\\n\\\\nGenerator\\\\n\\\\nGroundtruthMel-specGenerated Waveform\\\\n\\\\nOur Multi-task Training Framework\\\\n\\\\nAuxillaryTask\\\\n\\\\nGroundtruthMel-specGenerated WaveformReal WaveformStandard GAN-based Vocoder Training Pipeline\\\\n\\\\nused in other vocoder works like HiFi-GAN [4]. LJSpeech is a public single-speaker dataset with 13100 short English audio clips whose durations span from 1 second to 10 sec- onds. We use the default data split with 12950 training sam- ples and 150 validation samples. We use the same prepro- cessing configurations with HiFi-GAN, including 80 bands of mel-spectrograms as input and FFT size of 1024, window size of 1024, and hop size of 256 for conversion from waveform to mel-spectrograms.[4]\\\\n\\\\n3.1.2. Implementation details\\\\n\\\\nIllustration of our multi-tasking frameworks. Fig. 3. GAN-based Vocoder models [3, 4] follow an adversarial net- work (top) consisting of a generator that generates raw wave- forms from mel-spectrograms and a discriminator that aims to distinguish real from generated waveform samples. To in- corporate the auxiliary contrastive learning task, we propose a multi-tasking (bottom) framework, which we set the con- trastive task as additional learning objectives along with the original GAN optimization objectives. This framework ap- plies to both contrastive learning methods described in sec- tion 2.1 and 2.2.\\\\n\\\\ngenerator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:\\\\n\\\\nFor experimental comparison on audio quality, we choose the most powerful HiFi-GAN V1 and the most lightweight HiFi- GAN V3 as the baseline methods, and we use the same model architecture as the backbone to apply the contrastive tasks de- scribed in section 2.1 and 2.2. Under the multi-tasking frame- work, we train HiFi-GAN along with the contrastive learning methods with a batch size of 16, an AdamW optimizer, and a learning rate of 0.0002. For the following experiments on the full dataset, all models are trained for 400k steps (about 96 hours) on one Nvidia TITAN RTX GPU. The experiments on 20% of the dataset train for 300k steps (about 72 hours) on the same device, and those on 4% of the dataset train for 200k steps. The model inference time on GPU is about 70ms for V1 models and 32ms for V3 models.\\\\n\\\\nLG = Ladv + λf mLf m + λmelLmel + λclLcl\\\\n\\\\n(3)\\\\n\\\\nModel\\\\n\\\\nMAE MCD MOS (CI)\\\\n\\\\nGround Truth\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n4.32 (±0.05)\\\\n\\\\nLD = Ladv + IdiscλclLcl\\\\n\\\\nwhere LG is the total loss for the generator and LD is the total loss for the discriminator. Ladv is the adversarial loss, Lf m is the feature matching loss, and Lmel is the mel- spectrogram reconstruction loss in the original HiFi-GAN training pipeline. Lmel can be either of the contrastive loss described in section 2.1 or 2.2, and Idisc is an indicator of whether the latter is used. Each loss is weighted with a λ coefficient which can be set as hyperparameters. We use a λf m of 2, λmel of 45 from the HiFi-GAN setting [4] and a λcl of 1.\\\\n\\\\n(4)\\\\n\\\\n0.111 HiFi-GAN V1 + Mel CL 0.114 + Mel-Wave CL 0.113\\\\n\\\\n4.203 4.289 4.228\\\\n\\\\n4.21 (±0.05) 4.18 (±0.06) 4.20 (±0.05)\\\\n\\\\n0.203 HiFi-GAN V3 + Mel CL 0.204 + Mel-Wave CL 0.203\\\\n\\\\n7.786 7.766 7.723\\\\n\\\\n4.10 (±0.05) 4.13 (±0.07) 4.09 (±0.06)\\\\n\\\\nTable 1. Objective and subjective evaluation results for mod- els with mel-spectrogram contrastive loss (Mel CL) and mel- spectrogram contrastive loss (Mel-Wave CL). Models are trained on the full training set. CI is 95% confidence inter- val of the MOS score.\\\\n\\\\n3. EXPERIMENTS\\\\n\\\\n3.1. Experimental Setting\\\\n\\\\n3.1.3. Evaluation metrics\\\\n\\\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\\\n\\\\n3.1.1. Dataset\\\\n\\\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also\\\\n\\\\nTo objectively evaluate our models compared to the baseline, we measure the mean average error (MAE) and mel-cepstral distortion (MCD) [30] on mel-spectrograms. On both met- rics, lower scores indicate closer alignment with the ground truth. We also include a 5-scale mean opinion score (MOS) on audio quality as a subjective evaluation performed on 50 samples excluded from the training set.\\\\n\\\\nModel\\\\n\\\\nMAE ↓\\\\n\\\\nMCD ↓\\\\n\\\\nMOS ↑ (CI)\\\\n\\\\nGround Truth\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n4.32 (±0.05)\\\\n\\\\nHiFi-GAN V1 (20% data) + Mel CL (20% data) + Mel-Wave CL (20% data)\\\\n\\\\n0.113 (↑ 0.002) 0.116 (↑ 0.002) 0.113 (↑ 0.000)\\\\n\\\\n4.352 (↑ 0.149) 4.430 (↑ 0.139) 4.295 (↑ 0.067)\\\\n\\\\n4.13 (↓ 0.08) (±0.06) 4.11 (↓ 0.07) (±0.07) 4.16 (↓ 0.04) (±0.06)\\\\n\\\\nHifi-GAN V3 (20% data) + Mel CL (20% data) + Mel-Wave CL (20% data)\\\\n\\\\n0.212 (↑ 0.009) 0.207 (↑ 0.003) 0.207 (↑ 0.004)\\\\n\\\\n8.157 (↑ 0.371) 7.960 (↑ 0.206) 7.974 (↑ 0.251)\\\\n\\\\n3.88 (↓ 0.22) (±0.06) 3.95 (↓ 0.18) (±0.06) 4.04 (↓ 0.05) (±0.07)\\\\n\\\\nTable 2. Objective and subjective evaluation results for models trained with 20% of the training set. The number in parenthesis indicates the difference from the results when trained on the full dataset.\\\\n\\\\n3.2. Results\\\\n\\\\nWe present the results of models trained on full data with the multi-tasking framework in Table 1. Below, we refer Mel CL as the mel-spectrogram contrastive learning in section 2.1, and Mel-Wave CL as the mel-spectrogram waveform con- trastive learning in section 2.2. For V1 models, the baseline performs slightly better than the proposed methods by mar- gins of 0.02 on MAE, 0.025 on MCD, and 0.01 on MOS. For V3 models, on the objective tests, we observe that the model trained with mel-spectrogram contrastive loss has com- parable performance with the baseline, while the one trained with mel-spectrogram waveform contrastive loss achieves the highest scores on both metrics. The results show that our pro- posed methods have at least comparable performance to the baseline HiFi-GAN when training on the full dataset. On the subjective tests, the V3 model with Mel CL achieves the high- est MOS score, 0.03 above the V3 baseline. The model with Mel-Wave CL has a similar MOS score with the baseline on the full dataset. Overall, when trained on the full dataset, the proposed methods have limited gains on top of the baseline.\\\\n\\\\nTo investigate how each model performs under data lim- itation, we train the three models on 20% of the dataset and evaluate them with the same validation set. We present the results in Table 2. With less data, the baseline HiFi-GAN V3 suffers a significant performance degradation across all met- rics, including 0.371 on MCD and 0.22 on MOS. Meanwhile, the V3 model trained with Mel CL experiences an increase of 0.194 on MCD and a drop of 0.18 on MOS. The V3 model trained with Mel-Wave CL has an increase of 0.251 on MCD and a drop of only 0.05 on MOS. It suggests Mel-Wave CL is most resistant to data insufficiency. The two proposed meth- ods have comparable scores on the objective evaluation, but the model with Mel-Wave CL obtains a significantly higher score on the subjective test, 0.16 higher than the V3 base- line. The findings align with our hypothesized alleviation of discriminator overfitting by Mel-Wave CL, which is a more severe problem on the small training dataset. Both of the pro- posed methods perform substantially better than the baseline by 0.07 and 0.16 respectively.\\\\n\\\\nA similar trend exists in the HiFi-GAN V1 experiments, where Mel-Wave CL achieves the best scores and the least performance drop on all metrics. One slightly surprising find- ing is that the larger model V1 often experiences a smaller performance drop compared to the smaller model V3 when trained on 20% data. Typically, a larger model is expected to be more prone to overfitting when trained on less data, which should lead to a larger performance drop. In this specific case, however, HiFi-GAN V1 has a larger generator but the same discriminator as HiFi-GAN V3 [4], which is our suspected reason for the finding. Overall, the results show the benefits of additional supervision signals from contrastive learning in data-limited situations and the superior performance of Mel- Wave CL on a small dataset.\\\\n\\\\n4. CONCLUSION\\\\n\\\\nThis paper describes our proposed contrastive learning frame- work to improve GAN vocoders. Our results show the legacy of using contrastive learning as an auxiliary task that facili- tates vocoder training without adding more data or modifying model architecture. We demonstrate that the proposed frame- work is significant especially when training on limited data by extracting additional supervision signals and reducing dis- criminator overfitting.\\\\n\\\\nFor future work, we plan to repeat the experiments on dif- ferent model architectures and datasets to test our method’s generalizability. In particular, we want to test its extension to multi-speaker datasets, another domain where data insuffi- ciency is critical. We will also explore other metrics to evalu- ate the discriminator overfitting problem more holistically.\\\\n\\\\n5. REFERENCES\\\\n\\\\n[1] Haoming Guo, Gerald Friedland, and Gopala Krishna Anu- manchipalli, “Enhancing gan-based vocoders with contrastive learning,” M.S. thesis, EECS Department, University of Cali- fornia, Berkeley, May 2023.\\\\n\\\\n[2] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, “Generative adversarial networks,” 2014.\\\\n\\\\n[3] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Br´ebisson, Yoshua Bengio, and Aaron C Courville, “Melgan: Generative adversarial networks for conditional waveform synthesis,” in Advances in Neural Information Processing Systems, H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d\\'Alch´e-Buc, E. Fox, and R. Garnett, Eds. 2019, vol. 32, Curran Associates, Inc.\\\\n\\\\n[16] Jiachen Lian, Alexei Baevski, Wei-Ning Hsu, and Michael Auli, “Av-data2vec: Self-supervised learning of audio-visual speech representations with contextualized target representa- tions,” arXiv preprint arXiv:2302.06419, 2023.\\\\n\\\\n[17] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han, “Differentiable augmentation for data-efficient gan train- ing,” ArXiv, vol. abs/2006.10738, 2020.\\\\n\\\\n[4] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, “Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,” ArXiv, vol. abs/2010.05646, 2020.\\\\n\\\\n[5] Sang gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon, “BigVGAN: A universal neural vocoder with large-scale training,” in The Eleventh International Conference on Learning Representations, 2023.\\\\n\\\\n[6] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Repre- sentation learning with contrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\\\\n\\\\n[18] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang, “Regularizing generative adversarial networks under limited data,” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7917–7927, 2021.\\\\n\\\\n[19] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila, “Training generative ad- versarial networks with limited data,” in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 12104–12114, Curran Associates, Inc.\\\\n\\\\n[7] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” 2021.\\\\n\\\\n[8] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu- jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh- ioka, Xiong Xiao, et al., “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505– 1518, 2022.\\\\n\\\\n[20] Axel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas Geiger, “Projected gans converge faster,” CoRR, vol. abs/2111.01007, 2021.\\\\n\\\\n[21] Jongheon Jeong and Jinwoo Shin,\\\\n\\\\n“Training {gan}s with stronger augmentations via contrastive discriminator,” in In- ternational Conference on Learning Representations, 2021.\\\\n\\\\n[22] Yihao Xue, Kyle Whitecross, and Baharan Mirzasoleiman, “Investigating why contrastive learning benefits robustness against label noise,” in First Workshop on Pre-training: Per- spectives, Pitfalls, and Paths Forward at ICML 2022, 2022.\\\\n\\\\n[9] Aaqib Saeed, David Grangier, and Neil Zeghidour,\\\\n\\\\n“Con- trastive learning of general-purpose audio representations,” 2020.\\\\n\\\\n[23] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Xiaodong Song, “Using self-supervised learning can improve model robustness and uncertainty,” in Neural Infor- mation Processing Systems, 2019.\\\\n\\\\n[10] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feicht- enhofer, “Masked autoencoders that listen,” 2022.\\\\n\\\\n[11] Junrui Ni, Liming Wang, Heting Gao, Kaizhi Qian, Yang Zhang, Shiyu Chang, and Mark Hasegawa-Johnson, “Unsu- pervised text-to-speech synthesis by unsupervised automatic speech recognition,” arXiv preprint arXiv:2203.15796, 2022.\\\\n\\\\n[12] Jiachen Lian, Chunlei Zhang, and Dong Yu,\\\\n\\\\n“Robust dis- entangled variational speech representation learning for zero- shot voice conversion,” in ICASSP 2022-2022 IEEE Interna- tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 6572–6576.\\\\n\\\\n[13] Jiachen Lian, Chunlei Zhang, Gopala Krishna Anumanchipalli, and Dong Yu, “Towards improved zero-shot voice conversion arXiv preprint arXiv:2205.05227, with conditional dsvae,” 2022.\\\\n\\\\n[14] Jiachen Lian, Chunlei Zhang, Gopala Krishna Anumanchipalli, and Dong Yu, “Utts: Unsupervised tts with conditional disen- tangled sequential variational auto-encoder,” arXiv preprint arXiv:2206.02512, 2022.\\\\n\\\\n[24] Aritra Ghosh and Andrew Lan, “Contrastive learning improves model robustness under label noise,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR) Workshops, June 2021, pp. 2703–2708.\\\\n\\\\n[25] Kehan Wang, Seth Z. Zhao, David Chan, Avideh Zakhor, and John Canny, “Multimodal semantic mismatch detection in so- cial media posts,” in Proceedings of IEEE 24th International Workshop on Multimedia Signal Processing (MMSP), 2022.\\\\n\\\\n[26] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Represen- tation learning with contrastive predictive coding,” 2018.\\\\n\\\\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever, “Learning transferable visual models from nat- ural language supervision,” 2021.\\\\n\\\\n[28] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang, “Vision-language navigation with self-supervised auxiliary reasoning tasks,” 2019.\\\\n\\\\n[29] Keith Ito and Linda Johnson, “The lj speech dataset,” https: //keithito.com/LJ-Speech-Dataset/, 2017.\\\\n\\\\n[15] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrah- man Mohamed, “Learning audio-visual speech representa- tion by masked multimodal cluster prediction,” arXiv preprint arXiv:2201.02184, 2022.\\\\n\\\\n[30] Robert F. Kubichek, “Mel-cepstral distance measure for ob- jective speech quality assessment,” Proceedings of IEEE Pa- cific Rim Conference on Communications Computers and Sig- nal Processing, vol. 1, pp. 125–128 vol.1, 1993.\", metadata={\\'source\\': \\'C:\\\\\\\\Users\\\\\\\\jeeho\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\tmp4e3xm4ak\\\\\\\\tmp.pdf\\'})]'),\n",
       " Document(page_content=\"[Document(page_content='IEEE Copyright Notice\\\\n\\\\n©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\\\\n\\\\nThis work has been submitted to the IEEE International Conference on Acoustics, Speech and Signal Processing.\\\\n\\\\n3 2 0 2\\\\n\\\\np e S 6 2\\\\n\\\\n] S A . s s e e [\\\\n\\\\n1 v 4 2 2 5 1 . 9 0 3 2 : v i X r a\\\\n\\\\nCOLLABORATIVE WATERMARKING FOR ADVERSARIAL SPEECH SYNTHESIS\\\\n\\\\nLauri Juvela ∗\\\\n\\\\nXin Wang †\\\\n\\\\nAalto University, Espoo, Finland lauri.juvela@aalto.fi\\\\n\\\\nNational Institute of Informatics, Tokyo, Japan wangxin@nii.ac.jp\\\\n\\\\nABSTRACT\\\\n\\\\nAdvances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Re- cently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to gen- erated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech water- marking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis- tently improves detection performance over conventional classifier training. Furthermore, we demonstrate how collaborative training can be paired with augmentation strategies for added robustness against noise and time-stretching. Finally, listening tests demonstrate that collaborative training has little adverse effect on perceptual quality of vocoded speech.\\\\n\\\\nIndex Terms— Voice cloning, Generated speech detection, Wa-\\\\n\\\\ntermarking, HiFi GAN, ASVspoof\\\\n\\\\n1. INTRODUCTION\\\\n\\\\nnecessarily cover those from the attackers. Hence, the key question for the defender is how to develop a detection model on the basis of the limited training data and make it generalize to unseen deepfakes. Research outcomes from, for example, the ASVspoof [4] and ADD challenges [5], have demonstrated some deep learning-based de- tectors can detect certain unseen deepfakes in the benchmark datasets with error rates smaller than 5%. However, many detectors were found to be vulnerable to spurious features in the training set [4, 6, 7] and generalize poorly to data from different domains [8, 9]. Even though the generalization capability may be improved by including more diverse training data, the long term equilibrium of this adver- sarial game of passive audio deepfake detection is yet to be seen. Attackers can always create audio deepfake from newer and stronger speech generation models.\\\\n\\\\nGenerative adversarial networks (GANs) [10] take a mirrored perspective on playing the adversarial game of detection and gener- ation. In this setting, a Discriminator network attempts to classify between samples from the real and generated data distributions, while another network, the Generator synthesizes new samples and tries to spoof the discriminator into classifying the generated samples as real. The theoretical equilibrium for the GAN game is that the generator learns to match the real data distribution exactly, and discriminating between real and generated samples becomes impossible [10]. De- spite practical challenges with finite model capacity, numerics, and unstable training dynamics, GAN-based synthesis methods [11, 12] can be used for realistic speech waveform synthesis. In particular, HifiGAN [13] is widely used as a high-quality neural vocoder in text-to-speech synthesis and voice-conversion.\\\\n\\\\nModern speech synthesis systems have achieved nearly human level naturalness and are capable of zero-shot voice cloning from a few seconds of adaptation data [1]. Open-source implementations, shar- ing pre-trained models, and good software packaging has made voice cloning with TTS easily accessible also outside the research com- munity [2]. Furthermore, the use of voice cloning technology as a service (for a small monthly subscription fee) has recently emerged as a popular past-time on the internet. With this growing user base, the amount of generated speech in the wild is increasing, which poses a risk of casually created misinformation and malicious deepfakes.\\\\n\\\\nThe adversarial perspective remains highly relevant in defence against black-hat attack scenarios. However, the intended use of a generative model is often not malicious, and deceptive realism is merely a side-product of a high-quality system. In these use cases, the generative model provided is likely happy to comply to any regulation and make an active effort watermark their generated model outputs as such. The relevance of watermarking is amplified by the common use of pre-trained models and hosted services: if a model has built-in watermarking that is not trivial to remove and does not affect the perceptual quality of the output, most users will make an effort to remove watermark.\\\\n\\\\nResearch on audio deepfake detection focuses mostly on passive protection using machine learning methods, which is also referred to as speech anti-spoofing [3]. This scenario assumes that defenders have no prior knowledge of what attackers will use to generate the audio deepfake. The attackers can use any speech generation models to create the audio deepfake, while the defenders only have a limited number of audio deepfake types in the training set, which does not\\\\n\\\\n∗We acknowledge the computational resources provided by the Aalto\\\\n\\\\nScience-IT project.\\\\n\\\\n†This study is supported by JST CREST Grants JPMJCR18A6 and JP-\\\\n\\\\nMJCR20D3, MEXT KAKENHI Grants 21K17775 and 21H04906.\\\\n\\\\nThis paper proposes a collaborative training scheme that tasks a generative model to watermark its output to be more easily de- tectable by a specific classifier without degrading the perceptual quality. The experiments use HiFi-GAN [13] as the generative model and ASVspoof 2021 challenge [4] baseline countermeasure models as watermark detector models. Additionally, the detection perfor- mance under additive noise and time-stretching is shown to improve by differentiable augmentation during training. Results show that col- laborative training consistently improves the detection performance compared to the corresponding passively trained countermeasure model.\\\\n\\\\nTime stretchWaveformReal/FakeReal/FakeHiFi GAN\\\\n\\\\nCollaboratorReal/FakeWatermark detector models\\\\n\\\\nStop grad\\\\n\\\\nDiscriminator\\\\n\\\\nGenerator\\\\n\\\\nAugmentation\\\\n\\\\nNoise\\\\n\\\\nMel-Spectrogram\\\\n\\\\nObserver\\\\n\\\\nWatermarks can be evaluated using multiple characteristics, including robustness, perceptibility, and applications [14]. Some applications, such as proof-of-ownership or object identification, require a wa- termark payload with high enough bit count to encode sufficient information, but this paper focuses on an simple zero-bit watermark: watermark is present in synthetic speech and not present in natu- ral speech. Sometimes researchers use the terms watermarking and fingerprinting interchangeably. We use fingerprint to denote the sum- mary of the perceptible information in the carrier speech signal, such as linguistic content, expression, speaker identity, or acoustic chan- nel information. In contrast, a watermark aims to convey hidden information and not be perceptible to the human listener.\\\\n\\\\nAudio watermarking usually consists of an embedder and a de- tector. The embedder embeds a watermark or message, e.g., pseudo- random numbers or a hash, into the input audio, and the detector verifies the message’s existence in a watermarked audio. Typical algorithms using DSP include adding pseudo-noise in the tempo- ral or spectral domain (a.k.a spread spectrum watermarking [15]), phase coding [16], and echo hiding [17]. Some recent methods also implement the embedded and detector using DNNs [18, 19].\\\\n\\\\nFig. 1: A Generator can view detector models in three distinct roles: fool the Discriminator to produce more realistic samples, ignore the Observer, or help the Collaborator to extract a watermark from generated speech.\\\\n\\\\nA notable approach that combines DSP and statistics is patchwork watermarking [20]. When implemented in the spectral domain, this algorithm embeds a single bit m in the spectrum of an audio frame by selecting two sets of frequency bins and changing their amplitude values in opposite ways. Let sk be the spectral amplitude at the k-th frequency bin and A and B be the two sets of frequency bins. The algorithm can be written as\\\\n\\\\n2. Observer acts as a passive detector. Gradient flow from the Observer to Generator is detached. This corresponds to tradi- tional ASVspoof countermeasure training.\\\\n\\\\n3. Collaborator shares its objective with the Generator. The pair attempt to embed a watermark into the generated signal in a way that aids the classifier, but does not hinder Generator’s other objectives. Notably,\\\\n\\\\n3. Collaborator shares its objective with the Generator. The pair attempt to embed a watermark into the generated signal in a way that aids the classifier, but does not hinder Generator’s other objectives. Notably,\\\\n\\\\n(cid:40)\\\\n\\\\nsj ← s1−d sj ← s1+d\\\\n\\\\nsi ← s1+d si ← s1−d\\\\n\\\\n, ,\\\\n\\\\nif m = 1 if m = 0\\\\n\\\\n, ,\\\\n\\\\nj\\\\n\\\\ni\\\\n\\\\n,\\\\n\\\\n(1)\\\\n\\\\nthe general approach is not specific to GANs or ASVspoof detector models, and can be adapted to a wide range of detectors and generative models. For simplicity, we chose to limit the experiments to neural vocoding instead of end-to-end TTS. Previous research has demonstrated that training ASV Spoofing countermeasures on vocoded speech transfers well to TTS systems using the same vocoders [23].\\\\n\\\\nj\\\\n\\\\ni\\\\n\\\\nwhere i ∈ A, j ∈ B, and d is a strength parameter. The general idea to detect the single bit of message is to compare the mean values of the two sets\\\\n\\\\n(cid:40)\\\\n\\\\n(cid:80)\\\\n\\\\n(cid:80)\\\\n\\\\ni∈A si − 1\\\\n\\\\n(cid:98)m = 1, if 1 (cid:98)m = 0, else\\\\n\\\\nj∈B sj > 0\\\\n\\\\n|A|\\\\n\\\\n|B|\\\\n\\\\n.\\\\n\\\\n(2)\\\\n\\\\n3.1. Loss functions\\\\n\\\\nA larger strength d value increases the robustness against distortions but degrades the quality of the watermarked audio.\\\\n\\\\nGenerator and Discriminator loss functions and architectures follow the HiFi-GAN recipe [13]. We use the large Generator model configu- ration (V1) and base the experiments on the official implementation1. Denote generated speech signal as xgen = G(m), where G is the Generator model, and m is the mel-spectrum of the target waveform. Discriminator, D, is trained with the least-squares GAN loss function, with the target score at one for real samples and at zero for generated LD = E (cid:2)D((xreal) − 1)2 + D(xgen)2(cid:3) , where the expectation, E, is approximated by minibatch averages over batch elements and timesteps. Each of the sub-discriminators in the HiFi-GAN discriminator ensemble share the same objective. Sim- ilarly, the Generator loss functions are from HiFi GAN, comprising the adversarial loss\\\\n\\\\nFor generative models in the image domain, watermarking the training data with standard DSP techniques has been shown to trans- fer to GAN outputs [21]. Yu et al. [22] propose a jointly trained watermark encoder-decoder scheme for GANs. This is closely re- lated to our work, both in motivation (“from passive classifiers to proactive fingerprinting”), and implementation (their Generator acts as a watermark embedding model). Main differences are different domains and our work adds augmentation for robustness.\\\\n\\\\n(3)\\\\n\\\\n3. PROPOSED METHOD\\\\n\\\\nFigure 1 depicts an overview of the system. A Generator model takes a mel-spectrogram as input and outputs a corresponding synthetic speech waveform. Detector models all try to classify between real and generated speech, and the training dynamics change based on which role the classifier takes:\\\\n\\\\nLG-adv = E (cid:2)(D(xgen) − 1)2(cid:3) ,\\\\n\\\\n(4)\\\\n\\\\nfeature matching loss at each hidden activation Di(·)\\\\n\\\\n(cid:88)\\\\n\\\\nE (cid:2)(Di(xreal) − Di(xgen))2(cid:3) ,\\\\n\\\\nLG-FM =\\\\n\\\\n(5)\\\\n\\\\n1. Discriminator is adversarial to the Generator. The Generator attempts to fool the discriminator into classifying generated samples as real.\\\\n\\\\ni\\\\n\\\\n1https://github.com/jik876/hifi-gan\\\\n\\\\nand L1 regression loss on log-mel-spectrograms\\\\n\\\\nLG-mel = E [| log(M · |STFT(xreal)|) − log(M · |STFTxgen|) ] ,\\\\n\\\\nwhere M is a tensorized mel-filterbank matrix.\\\\n\\\\n(6)\\\\n\\\\nand 1.1 and kept constant for each mini-batch. Second, additive noise samples are randomly drawn from the MUSAN database [29] noise subset. For simplicity, we keep the noise level constant at 10 dB SNR. More fine-grained analysis on the effect of varying noise levels is left as future work.\\\\n\\\\nThe watermark detector model, WM , has exactly the same ob-\\\\n\\\\njective as D: assign a high score to xreal and a low score to xgen\\\\n\\\\nLWM = E (cid:2)WM ((xreal) − 1)2 + WM (xgen)2(cid:3) .\\\\n\\\\n(7)\\\\n\\\\n4. EXPERIMENTS\\\\n\\\\n4.1. Datasets\\\\n\\\\nG can either share the WM objective or ignore it. These two scenar- ios are called Collaborator and Observer, respectively.\\\\n\\\\nIn practice, the system is trained by alternating minibatches that switch between the D objectives and the joint objective of G and WM . In Observer mode, the gradient flow from WM to G is de- tached and LWM has no effect on G.\\\\n\\\\n3.2. Watermark detector models\\\\n\\\\nBoth ASVspoof 2021 baseline models are designed to operate at at 16 kHz sample rate, whereas the HiFi GAN defaults to 22.05 kHz sample rate. To maintain compatibility with pre-trained models, we use a differentiable resampling layer between the Generator and watermark detector models. The resampler uses a Hann-windowed sinc interpolation filter with six filter taps.\\\\n\\\\nThe Voice Cloning Toolkit (VCTK) [30] corpus is used for all the speech data in the experiments. VCTK has a data split protocol for ASVspoof aimed at evaluation purposes [4], but we deemed the training set to small for training the synthesis system. Instead, we opted for a custom 80-10-10% split to training, validation, and test sets. Split details are provided with the source code. Each subset has distinct speakers, but has overlaps in text content. Noise data for augmentation consists of the noise subset from the MUSAN database [29]. We apply a 80-10-10% split and use unseen noise samples during testing.\\\\n\\\\n4.2. Training details\\\\n\\\\nPre-trained models use batch normalization in evaluation mode with stored statistics. For joint training, we removed batch normal- ization layers after identifying they led to poor generalization with collaborative training. Recent research has found that removing batch normalization boosts adversarial training [24], and we speculate the underlying cause may be related due to similar mathematics of adver- sarial and collaborative training.\\\\n\\\\nLFCC-LCNN is the first detector used in this study, as the base- line of the ASVspoof 2021 challenge2. Its front end extracts linear frequency cepstrum coefficients (LFCC) [25], which is similar to the Mel frequency cepstrum coefficients but uses filters placed in equal sizes on a linear scale. The frame length and hop size are 20 and 10 ms, respectively. The FFT size is 1,024, and the number of dimensions is 60, including delta and delta-delta components. The maximum frequency covered by the filter is half of the Nyquist fre- quency. The back end is a light convolution neural network (LCNN) [26], followed by two recurrent layers using long-short term memory units, global average pooling, and a linear output layer [27].\\\\n\\\\nFollowing HiFi-GAN, we use the AdamW optimizer with learning rate 2e-4, β1 = 0.8, β2 = 0.99 and learning rate decay with exponential decay factor 0.999. Training segements are padded or randomly cropped to 8,192 samples with LFCC-LCNN and 65,536 samples with RawNet. In Collaborator models, the gradients are allowed to flow back from the WM to G, while in Observer models the gradient is detached. Each model configuration is trained for 50 epochs, amounting to approximately 110k iterations. Note that the HiFi-GAN paper [13] reports training for 2.5M iterations, which does yield higher synthesis quality. This work focuses on demonstrating the relative benefit of collaborative training over baseline observer training in many scenarios. We believe the current setup offers a reasonable trade-off between the computational cost of training many systems, and the perceptual quality of generated speech. Models were trained on a mixture of GPUs consisting of NVIDIA A100, V100 and P100 models.\\\\n\\\\n4.3. Patchwork watermark baseline\\\\n\\\\nRawNet2 is another ASVspoof 2021 baseline used in this study is [28]. RawNet2 uses convolution (in DNNs) to implement the dif- ferentiable band-pass filters. The convolution kernel, or the filter coefficients, are pre-calculated in the same manner as the windowed- sinc filters, and the cut-off frequencies are co-located with the filter bank in MFCC. The input waveform is processed by the convolution layer and transformed by multiple blocks with 2D convolution, max pooling, and filter-wise feature map scaling. The hidden features are then processed by a recurrent layer with gated recurrent units and a linear output layer. Its configuration follows the official implementa- tion, except that the batch normalization layer is removed. The input waveform is padded or truncated to a fixed length of 65,536.\\\\n\\\\nAs a DSP baseline, patchwork watermarking is applied to vocoded speech from a HiFi GAN trained without detector collaboration. The patchwork watermarking algorithm described in Section 2 is included for reference. The open-sourced toolkit Audiowmark 3 is used due to its high-quality implementation with error-correction in watermarking detection. A fixed-length 128 bit text string is watermarked. The hyper-parameter d was selected for each test utterance through grid search. Given the range between 0.01 and 0.2, the value of d was decided so that the detector outputs the correct watermark without other hypothesis. During test conditions, the watermarked audio is added with noise or stretched before sent to the watermark detector.\\\\n\\\\n4.4. Listening test\\\\n\\\\n3.3. Differentiable augmentation\\\\n\\\\nWe apply two differentiable augmentation techniques to improve the robustness of the proposed collaborative watermarking method. First, time-stretching is implemented using linear interpolation. The time- scale factor is randomized uniformly between time-scale factors 0.9\\\\n\\\\nWe conducted a mean opinion score (MOS) test to evaluate the per- ceptual effect of the proposed method. Listeners were asked to rate the naturalness of the presented speech samples on a five-point scale ranging from 1 (bad) to 5 (excellent). The listening test stimuli con- sist of 50 utterances selected randomly from test set. After pooling\\\\n\\\\n2https://github.com/asvspoof-challenge/2021\\\\n\\\\n3https://github.com/swesterfeld/audiowmark\\\\n\\\\nTable 1: EER (%) of experiment systems in clean, stretch, noise, and stretch plus noise (S+N) testing conditions. A darker cell color indicates a higher EER value. Each EER was averaged over 20 independent rounds of generation and evaluation. Error rates (%) of spectral-domain patchwork watermarking are shown at the bottom for reference.\\\\n\\\\nClean\\\\n\\\\nStretch\\\\n\\\\nNoise\\\\n\\\\nS+N\\\\n\\\\nSpectral patchwork watermarking\\\\n\\\\n0.21\\\\n\\\\n32.65\\\\n\\\\n85.29\\\\n\\\\n98.25\\\\n\\\\nWM configuration\\\\n\\\\nLFCC-LCNN\\\\n\\\\nRawNet\\\\n\\\\nTraining\\\\n\\\\nAugmentation\\\\n\\\\nRole\\\\n\\\\nClean\\\\n\\\\nStretch\\\\n\\\\nNoise\\\\n\\\\nS + N\\\\n\\\\nClean\\\\n\\\\nStretch\\\\n\\\\nNoise\\\\n\\\\nS + N\\\\n\\\\nJoint\\\\n\\\\nNone\\\\n\\\\ncollaborator observer\\\\n\\\\n1.61 3.46\\\\n\\\\n2.95 5.13\\\\n\\\\n34.66 37.87\\\\n\\\\n42.90 46.20\\\\n\\\\n0.12 0.34\\\\n\\\\n43.25 17.86\\\\n\\\\n44.33 35.35\\\\n\\\\n48.62 49.65\\\\n\\\\nStretch + noise\\\\n\\\\ncollaborator observer\\\\n\\\\n1.05 3.73\\\\n\\\\n1.38 4.24\\\\n\\\\n15.25 27.72\\\\n\\\\n34.94 41.35\\\\n\\\\n1.36 3.72\\\\n\\\\n2.33 4.87\\\\n\\\\n4.03 13.31\\\\n\\\\n54.10 46.48\\\\n\\\\nPre-trained\\\\n\\\\nNone\\\\n\\\\ncollaborator observer\\\\n\\\\n17.73 49.47\\\\n\\\\n20.87 49.50\\\\n\\\\n42.13 49.05\\\\n\\\\n45.19 49.56\\\\n\\\\n10.64 47.76\\\\n\\\\n21.36 48.35\\\\n\\\\n28.32 48.55\\\\n\\\\n51.48 50.15\\\\n\\\\nStretch + noise\\\\n\\\\ncollaborator\\\\n\\\\n32.83\\\\n\\\\n32.79\\\\n\\\\n40.79\\\\n\\\\n44.37\\\\n\\\\n45.91\\\\n\\\\n46.75\\\\n\\\\n47.18\\\\n\\\\n48.40\\\\n\\\\nTable 2: Subjective MOS results with confidence interval (95%). Proposed models use WM as collaborator.\\\\n\\\\nthe ASVspoof pre-trained baselines, which have not been trained on HiFi-GAN vocoded speech.\\\\n\\\\nRef.\\\\n\\\\nd e s o p o r P\\\\n\\\\nNatural recording Spectral patchwork Baseline HiFi-GAN\\\\n\\\\nTraining\\\\n\\\\nAugmentation\\\\n\\\\nJoint\\\\n\\\\nNone Stretch + noise\\\\n\\\\nPre-trained\\\\n\\\\nNone Stretch + noise\\\\n\\\\n4.13 ± 0.11 3.49 ± 0.13 3.54 ± 0.13\\\\n\\\\nLFCC-LCNN\\\\n\\\\n3.38 ± 0.17 3.46 ± 0.13\\\\n\\\\n3.58 ± 0.13 3.46 ± 0.14\\\\n\\\\nRawNet\\\\n\\\\n3.59 ± 0.13 3.60 ± 0.13\\\\n\\\\n3.51 ± 0.13 3.67 ± 0.12\\\\n\\\\nThe DSP-based watermarking algorithm achieves a low error rate in the clean condition since the strength d was decided to allow perfect watermark detection in this condition. However, there are a few utterances in which the embedded watermark failed to be detected given the largest d. Its error rate increases when the watermarked audio is stretched, even when the provided compensation for playback was used. The performance degrades dramatically when additive noise is applied. Note that the patchwork baseline is not one-to-one comparable to the detector models, since baseline watermark uses a 128-bit payload and does not output detection scores required for EER calculation.\\\\n\\\\ntogether test utterances from each system, the stimuli were randomly batched to 100-sample listening sessions, and each batch was rated by at least five listeners on the Prolific crowd-sourcing platform. After screening, 3884 total ratings were used in the analysis. Table 2 shows MOS values with t-statistic based confidence intervals. Differences between vocoded systems are not statistically significant.\\\\n\\\\n5. RESULTS AND DISCUSSION\\\\n\\\\nA central limitation of the current experimental setup is the focus on neural vocoding. Real-world voice cloning applications use a full text-to-speech or voice conversion system. Nevertheless, we are optimistic on the transferability of training on vocoded speech based on recent results in the ASVspoof scenario [23]. However, security critical applications still need to follow an adversarial countermeasure protocol. Collaborative watermarking is only useful when the gener- ative model user has an incentive to watermark their model outputs. Furthermore, collaborative training is not limited to GANs. Any deep generative model with differentiable sampling (including diffusion models [31, 32]) can collaborate with a detector model to improve the odds of detection.\\\\n\\\\nTable 1 displays test set equal error rates (EER) for a range of systems. When training watermark detector models jointly with the generator, collaborative training consistently outperforms conventional observer training. This remains consistent over the detector model type, op- tional augmentation during training, and different test conditions, from clean to time-stretching and/or additive noise. All LFCC-LCNN configurations struggle with addivive noise, whereas collaborative training with RawNet and data augmentation still performs well under noise (EER 4.03). However, combining noise with time-stretching remains a challenging for both LFCC-LCNN and RawNet detectors, even when the models were trained with matching data augmentation. Pre-trained models struggle as watermark detectors, as listed in the bottom part of Table 1. In collaborative training, the synthesis model manages to embed some detectable information in clean con- ditions, but detection performance deteriorates when noise is added. In a zero-shot scenario (i.e., pre-trainer observer) the performance is near chance level for both LFCC-LCNN and RawNet. It appears that the Generator’s attempt to fool the Discriminator transfers to\\\\n\\\\n6. CONCLUSIONS\\\\n\\\\nThis paper proposes synthetic speech watermarking scheme based on collaborative training between a generative synthesis model and a watermark detector. The results show that collaborative training consistently improves detection performance compared to baseline passive countermeasure training. Source code and demonstration samples are available for readers4. Future work includes extending the study to full text-to-speech voice cloning systems, and developing specialized architectures for the speech watermarking task. Further, more informative watermarks are appealing: it would be useful to know who generated the samples, or what data was used to train the model.\\\\n\\\\n4https://ljuvela.github.io/CollaborativeWatermarkingDemo/\\\\n\\\\n7. REFERENCES\\\\n\\\\n[1] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang, I. Lopez Moreno, Y. Wu, et al., “Transfer learning from speaker verification to multispeaker text-to-speech synthesis,” Advances in neural information processing systems, vol. 31, 2018.\\\\n\\\\n[15] I. J. Cox, J. Kilian, T. Leighton, and T. Shamoon, “Secure spread spectrum watermarking for images, audio and video,” in Proc. ICIP, 1996, vol. 3, pp. 243–246.\\\\n\\\\n[16] W. Bender, D. Gruhl, N. Morimoto, and A. Lu, “Techniques for data hiding,” IBM systems journal, vol. 35, no. 3.4, pp. 313–336, 1996.\\\\n\\\\n[2] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. Gölge, and M. A. Ponti, “YourTTS: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone,” in Inter- national Conference on Machine Learning. PMLR, 2022, pp. 2709–2720.\\\\n\\\\n[3] Z. Wu, N. Evans, T. Kinnunen, J. Yamagishi, F. Alegre, and H. Li, “Spoofing and countermeasures for speaker verification: A survey,” Speech Communication, vol. 66, pp. 130–153, feb 2015.\\\\n\\\\n[17] D. Gruhl, A. Lu, and W. Bender, “Echo hiding,” in Information Hiding: First International Workshop Cambridge, UK, May 30–June 1, 1996 Proceedings 1. Springer, 1996, pp. 295–315.\\\\n\\\\n[18] K. Pavlovi´c, S. Kovaˇcevi´c, I. Djurovi´c, and A. Wojciechowski, “Robust speech watermarking by a jointly trained embedder and detector using a DNN,” Digital Signal Processing, vol. 122, pp. 103381, 2022.\\\\n\\\\n[19] G. Chen, Y. Wu, S. Liu, T. Liu, X. Du, and F. Wei, “Wav- mark: Watermarking for audio generation,” arXiv preprint arXiv:2308.12770, 2023.\\\\n\\\\n[4] X. Liu, X. Wang, M. Sahidullah, J. Patino, H. Delgado, T. Kin- nunen, M. Todisco, J. Yamagishi, N. Evans, A. Nautsch, and K. A. Lee, “ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild,” IEEE/ACM Transactions on Au- dio, Speech, and Language Processing, vol. 31, pp. 2507–2522, 2023.\\\\n\\\\n[20] M. Steinebach, Digitale Wasserzeichen fuer Audiodaten, Shaker,\\\\n\\\\n2004.\\\\n\\\\n[21] N. Yu, V. Skripniuk, S. Abdelnabi, and M. Fritz, “Artificial fin- gerprinting for generative models: Rooting deepfake attribution in training data,” in Proc. ICCV, 2021, pp. 14448–14457.\\\\n\\\\n[5] J. Yi, R. Fu, J. Tao, S. Nie, H. Ma, C. Wang, T. Wang, Z. Tian, Y. Bai, C. Fan, S. Liang, S. Wang, S. Zhang, X. Yan, L. Xu, Z. Wen, and H. Li, “ADD 2022: The first Audio Deep Synthesis Detection Challenge,” in Proc. ICASSP, May 2022, pp. 9216– 9220.\\\\n\\\\n[6] N. Müller, F. Dieckmann, P. Czempin, R. Canals, K. Böttinger, and J. Williams, “Speech is Silver, Silence is Golden: What do ASVspoof-trained Models Really Learn?,” in Proc. ASVspoof Challenge workshop, 2021, pp. 55–60.\\\\n\\\\n[7] Y. Zhang, W. Wang, and P. Zhang, “The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System,” in Proc. Interspeech, 2021, pp. 4279–4283.\\\\n\\\\n[8] D. Paul, M. Sahidullah, and G. Saha, “Generalization of spoof- ing countermeasures: A case study with ASVspoof 2015 and BTAS 2016 corpora,” in Proc. ICASSP, 2017, pp. 2047–2051.\\\\n\\\\n[9] N. M. Müller, P. Czempin, F. Dieckmann, A. Froghyar, and K. Böttinger, “Does Audio Deepfake Detection Generalize?,” Proc. Interspeech, pp. 2783–2787, 2022.\\\\n\\\\n[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde- Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 2014.\\\\n\\\\n[11] L. Juvela, B. Bollepalli, J. Yamagishi, and P. Alku, “GELP: GAN-excited linear prediction for speech synthesis from mel- spectrogram,” in Proc. Interspeech, 2019, pp. 694–698.\\\\n\\\\n[22] N. Yu, V. Skripniuk, D. Chen, L. S. Davis, and M. Fritz, “Re- sponsible disclosure of generative models using scalable finger- printing,” in Proc. ICLR, 2022.\\\\n\\\\n[23] X. Wang and J. Yamagishi, “Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders,” in Proc. ICASSP, 2023, pp. 1–5.\\\\n\\\\n[24] H. Wang, A. Zhang, S. Zheng, X. Shi, M. Li, and Z. Wang, “Removing batch normalization boosts adversarial training,” in International Conference on Machine Learning. PMLR, 2022, pp. 23433–23445.\\\\n\\\\n[25] M. Sahidullah, T. Kinnunen, and C. Hanilçi, “A comparison of features for synthetic speech detection,” in Proc. Interspeech, 2015, pp. 2087–2091.\\\\n\\\\n[26] X. Wu, R. He, Z. Sun, and T. Tan, “A light CNN for deep face representation with noisy labels,” IEEE Transactions on Information Forensics and Security, vol. 13, no. 11, pp. 2884– 2896, 2018.\\\\n\\\\n[27] X. Wang and J. Yamagishi, “A comparative study on recent neural spoofing countermeasures for synthetic speech detection,” in Proc. Interspeech, 2021, pp. 4259–4263.\\\\n\\\\n[28] H. Tak, J. Patino, M. Todisco, A. Nautsch, N. Evans, and A. Larcher, “End-to-end anti-spoofing with RawNet2,” in Proc. ICASSP, 2020, pp. 6369–6373.\\\\n\\\\n[29] D. Snyder, G. Chen, and D. Povey, “MUSAN: A Music, Speech,\\\\n\\\\nand Noise Corpus,” 2015.\\\\n\\\\n[12] R. Yamamoto, E. Song, and J.-M. Kim, “Probability density distillation with generative adversarial networks for high-quality parallel waveform generation,” in Proc. Interspeech, 2019, pp. 699–703.\\\\n\\\\n[13] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adver- sarial networks for efficient and high fidelity speech synthesis,” Advances in Neural Information Processing Systems, vol. 33, pp. 17022–17033, 2020.\\\\n\\\\n[14] F. Petitcolas, “Watermarking schemes evaluation,” IEEE Signal\\\\n\\\\n[30] J. Yamagishi, C. Veaux, and K. MacDonald, “CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92),” 2019.\\\\n\\\\n[31] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan, “WaveGrad: Estimating gradients for waveform generation,” in Proc. ICLR, 2021.\\\\n\\\\n[32] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Dif- fWave: A versatile diffusion model for audio synthesis,” in Proc. ICLR, 2021.\\\\n\\\\nProcessing Magazine, vol. 17, no. 5, pp. 58–64, 2000.', metadata={'source': 'C:\\\\\\\\Users\\\\\\\\jeeho\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\tmpw_bc0a72\\\\\\\\tmp.pdf'})]\"),\n",
       " Document(page_content=\"[Document(page_content='NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS\\\\n\\\\nJonathan Shen1, Ruoming Pang1, Ron J. Weiss1, Mike Schuster1, Navdeep Jaitly1, Zongheng Yang∗2, Zhifeng Chen1, Yu Zhang1, Yuxuan Wang1, RJ Skerry-Ryan1, Rif A. Saurous1, Yannis Agiomyrgiannakis1, and Yonghui Wu1\\\\n\\\\n1Google, Inc., 2University of California, Berkeley, {jonathanasdf,rpang,yonghui}@google.com\\\\n\\\\n8 1 0 2\\\\n\\\\nb e F 6 1\\\\n\\\\n] L C . s c [\\\\n\\\\n2 v 4 8 8 5 0 . 2 1 7 1 : v i X r a\\\\n\\\\nABSTRACT\\\\n\\\\nThis paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a mod- iﬁed WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for profes- sionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the im- pact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a signiﬁcant reduction in the size of the WaveNet architecture.\\\\n\\\\nIndex Terms— Tacotron 2, WaveNet, text-to-speech\\\\n\\\\n1. INTRODUCTION\\\\n\\\\nGenerating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the ﬁeld. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds mufﬂed and unnatural compared to human speech.\\\\n\\\\nWaveNet [8], a generative model of time domain waveforms, pro- duces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require signiﬁcant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide).\\\\n\\\\nTacotron [12], a sequence-to-sequence architecture [13] for pro- ducing magnitude spectrograms from a sequence of characters, sim- pliﬁes the traditional speech synthesis pipeline by replacing the pro- duction of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Grifﬁn-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As\\\\n\\\\n∗Work done while at Google.\\\\n\\\\nthe authors note, this was simply a placeholder for future neural vocoder approaches, as Grifﬁn-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet.\\\\n\\\\nIn this paper, we describe a uniﬁed, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modiﬁed WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and correspond- ing speech waveforms, our model learns to synthesize natural sound- ing speech that is difﬁcult to distinguish from real human speech.\\\\n\\\\nDeep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs signiﬁcantly.\\\\n\\\\n2. MODEL ARCHITECTURE\\\\n\\\\nOur proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modiﬁed version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames.\\\\n\\\\n2.1. Intermediate Feature Representation\\\\n\\\\nIn this work we choose a low-level acoustic representation: mel- frequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high ﬁdelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17].\\\\n\\\\n1\\\\n\\\\nWhile linear spectrograms discard phase information (and are therefore lossy), algorithms such as Grifﬁn-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectro- grams discard even more information, presenting a challenging in- verse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lower- level acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. In- deed, we will show that it is possible to generate high quality audio from mel spectrograms using a modiﬁed WaveNet architecture.\\\\n\\\\n2.2. Spectrogram Prediction Network\\\\n\\\\nAs in Tacotron, mel spectrograms are computed through a short- time Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in signiﬁcantly more pronunciation issues.\\\\n\\\\nStop Token\\\\n\\\\nBidirectional LSTM\\\\n\\\\n2 Layer Pre-Net\\\\n\\\\nCharacter Embedding\\\\n\\\\nInput Text\\\\n\\\\nLocation Sensitive Attention\\\\n\\\\n2 LSTM Layers\\\\n\\\\nWaveform Samples\\\\n\\\\nWaveNet MoL\\\\n\\\\n3 Conv Layers\\\\n\\\\nMel Spectrogram\\\\n\\\\nLinear Projection\\\\n\\\\nLinear Projection\\\\n\\\\n5 Conv Layer Post-Net\\\\n\\\\nFig. 1. Block diagram of the Tacotron 2 system architecture.\\\\n\\\\nWe transform the STFT magnitude to the mel scale using an 80 channel mel ﬁlterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the ﬁlterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain.\\\\n\\\\nThe network is composed of an encoder and a decoder with atten- tion. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolu- tional layers each containing 512 ﬁlters with shape 5 × 1, i.e., where each ﬁlter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N -grams) in the input character sequence. The output of the ﬁnal convolutional layer is passed into a single bi-directional [19] LSTM [20] layer containing 512 units (256 in each direction) to generate the encoded features.\\\\n\\\\nThe encoder output is consumed by an attention network which summarizes the full encoded sequence as a ﬁxed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and lo- cation features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution ﬁlters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is ﬁrst passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The pre- net output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each\\\\n\\\\npost-net layer is comprised of 512 ﬁlters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the ﬁnal layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difﬁcult to train and they did not lead to better sounding samples.\\\\n\\\\nIn parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a ﬁxed duration. Speciﬁcally, generation completes at the ﬁrst frame for which this probability exceeds a threshold of 0.5.\\\\n\\\\nThe convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder.\\\\n\\\\nIn contrast to the original Tacotron, our model uses simpler build- ing blocks, using vanilla LSTM and convolutional layers in the en- coder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame.\\\\n\\\\n2.3. WaveNet Vocoder\\\\n\\\\nWe use a modiﬁed version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers.\\\\n\\\\nInstead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10- component mixture of logistic distributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed\\\\n\\\\nby a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.\\\\n\\\\n3. EXPERIMENTS & RESULTS\\\\n\\\\n3.1. Training Setup\\\\n\\\\nfeatures[8] with similar modiﬁcations to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Grifﬁn-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We ﬁnd that the proposed system signiﬁcantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio. †\\\\n\\\\nOur training process involves ﬁrst training the feature prediction network on its own, followed by training a modiﬁed WaveNet inde- pendently on the outputs generated by the ﬁrst network.\\\\n\\\\nTo train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with β1 = 0.9, β2 = 0.999, (cid:15) = 10−6 and a learning rate of 10−3 exponentially decaying to 10−5 starting after 50,000 iterations. We also apply L2 regularization with weight 10−6. We then train our modiﬁed WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with β1 = 0.9, β2 = 0.999, (cid:15) = 10−8 and a ﬁxed learning rate of 10−4. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text.\\\\n\\\\nSystem\\\\n\\\\nMOS\\\\n\\\\nParametric Tacotron (Grifﬁn-Lim) Concatenative WaveNet (Linguistic) Ground truth\\\\n\\\\n3.492 ± 0.096 4.001 ± 0.087 4.166 ± 0.091 4.341 ± 0.051 4.582 ± 0.053\\\\n\\\\nTacotron 2 (this paper)\\\\n\\\\n4.526 ± 0.066\\\\n\\\\nTable 1. Mean Opinion Score (MOS) evaluations with 95% conﬁ- dence intervals computed from the t-distribution for various systems.\\\\n\\\\nWe also conduct a side-by-side evaluation between audio synthe- sized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of −0.270 ± 0.155 shows that raters have a small but statistically signiﬁcant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference.\\\\n\\\\n3.2. Evaluation\\\\n\\\\nWhen generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing conﬁguration used for training.\\\\n\\\\nFig. 2. Synthesized vs. ground truth: 800 ratings on 100 items.\\\\n\\\\nWe randomly selected 100 ﬁxed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them.\\\\n\\\\nNote that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inﬂated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful.\\\\n\\\\nTable 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic\\\\n\\\\nWe ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as em- phasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling.\\\\n\\\\nFinally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.148 ± 0.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 ± 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insigniﬁcant preference towards our\\\\n\\\\n†Samples available at https://google.github.io/tacotron/publications/tacotron2.\\\\n\\\\nresults by 0.142 ± 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difﬁculties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage.\\\\n\\\\n3.3. Ablation Studies\\\\n\\\\nprocessing network to incorporate past and future frames after decod- ing to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429 ± 0.071, compared to 4.526 ± 0.066 with it, meaning that em- pirically the post-net is still an important part of the network design.\\\\n\\\\n3.3.1. Predicted Features versus Ground Truth\\\\n\\\\nWhile the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2.\\\\n\\\\nSynthesis\\\\n\\\\nTraining\\\\n\\\\nPredicted\\\\n\\\\nGround truth\\\\n\\\\nPredicted Ground truth\\\\n\\\\n4.526 ± 0.066 4.362 ± 0.066\\\\n\\\\n4.449 ± 0.060 4.522 ± 0.055\\\\n\\\\nTable 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms.\\\\n\\\\n3.3.4. Simplifying WaveNet\\\\n\\\\nA deﬁning feature of WaveNet is its use of dilated convolution to increase the receptive ﬁeld exponentially with the number of layers. We evaluate models with varying receptive ﬁeld sizes and number of layers to test our hypothesis that a shallow network with a small recep- tive ﬁeld may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we ﬁnd that our model can generate high- quality audio using as few as 12 layers with a receptive ﬁeld of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results conﬁrm the observations in [9] that a large receptive ﬁeld size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity.\\\\n\\\\nAs expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from pre- dicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features.\\\\n\\\\nOn the other hand, if we eliminate dilated convolutions altogether, the receptive ﬁeld becomes two orders of magnitude smaller than the baseline and the quality degrades signiﬁcantly even though the stack is as deep as the baseline model. This indicates that the model requires sufﬁcient context at the time scale of waveform samples in order to generate high quality sound.\\\\n\\\\n3.3.2. Linear Spectrograms\\\\n\\\\nTotal layers\\\\n\\\\nNum cycles\\\\n\\\\nDilation cycle size\\\\n\\\\nReceptive ﬁeld (samples / ms)\\\\n\\\\nMOS\\\\n\\\\nInstead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Grifﬁn-Lim.\\\\n\\\\n30 24 12 30\\\\n\\\\n3 4 2 30\\\\n\\\\n10 6 6 1\\\\n\\\\n6,139 / 255.8 505 / 21.0 253 / 10.5 61 / 2.5\\\\n\\\\n4.526 ± 0.066 4.547 ± 0.056 4.481 ± 0.059 3.930 ± 0.076\\\\n\\\\nSystem\\\\n\\\\nMOS\\\\n\\\\nTable 4. WaveNet with various layer and receptive ﬁeld sizes.\\\\n\\\\nTacotron 2 (Linear + G-L) Tacotron 2 (Linear + WaveNet) Tacotron 2 (Mel + WaveNet)\\\\n\\\\n3.944 ± 0.091 4.510 ± 0.054 4.526 ± 0.066\\\\n\\\\n4. CONCLUSION\\\\n\\\\nTable 3. Comparison of evaluated MOS for Grifﬁn-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel spectrograms as conditioning inputs to WaveNet.\\\\n\\\\nAs noted in [10], WaveNet produces much higher quality audio compared to Grifﬁn-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better choice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work.\\\\n\\\\nThis paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modiﬁed WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.\\\\n\\\\n5. ACKNOWLEDGMENTS\\\\n\\\\n3.3.3. Post-Processing Network\\\\n\\\\nSince it is not possible to use the information of predicted future frames before they have been decoded, we use a convolutional post-\\\\n\\\\nThe authors thank Jan Chorowski, Samy Bengio, A¨aron van den Oord, and the WaveNet and Machine Hearing teams for their helpful discussions and advice, as well as Heiga Zen and the Google TTS team for their feedback and assistance with running evaluations. The authors are also grateful to the very thorough reviewers.\\\\n\\\\n6. REFERENCES\\\\n\\\\n[1] P. Taylor, Text-to-Speech Synthesis, Cambridge University\\\\n\\\\nPress, New York, NY, USA, 1st edition, 2009.\\\\n\\\\n[17] S. Davis and P. Mermelstein, “Comparison of parametric repre- sentations for monosyllabic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 28, no. 4, pp. 357 – 366, 1980.\\\\n\\\\n[2] A. J. Hunt and A. W. Black, “Unit selection in a concatenative speech synthesis system using a large speech database,” in Proc. ICASSP, 1996, pp. 373–376.\\\\n\\\\n[18] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proc. ICML, 2015, pp. 448–456.\\\\n\\\\n[3] A. W. Black and P. Taylor, “Automatically clustering simi- lar units for unit selection in speech synthesis,” in Proc. Eu- rospeech, September 1997, pp. 601–604.\\\\n\\\\n[19] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, Nov. 1997.\\\\n\\\\n[4] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Ki- tamura, “Speech parameter generation algorithms for HMM- based speech synthesis,” in Proc. ICASSP, 2000, pp. 1315– 1318.\\\\n\\\\n[5] H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric speech synthesis,” Speech Communication, vol. 51, no. 11, pp. 1039–1064, 2009.\\\\n\\\\n[6] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech synthesis using deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.\\\\n\\\\n[7] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura, “Speech synthesis based on hidden Markov models,” Proc. IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.\\\\n\\\\n[8] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu, “WaveNet: A generative model for raw audio,” CoRR, vol. abs/1609.03499, 2016.\\\\n\\\\n[9] S. ¨O. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gib- iansky, Y. Kang, X. Li, J. Miller, J. Raiman, S. Sengupta, and M. Shoeybi, “Deep voice: Real-time neural text-to-speech,” CoRR, vol. abs/1702.07825, 2017.\\\\n\\\\n[20] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\\\\n\\\\n[21] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben- gio, “Attention-based models for speech recognition,” in Proc. NIPS, 2015, pp. 577–585.\\\\n\\\\n[22] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine transla- tion by jointly learning to align and translate,” in Proc. ICLR, 2015.\\\\n\\\\n[23] C. M. Bishop, “Mixture density networks,” Tech. Rep., 1994.\\\\n\\\\n[24] M. Schuster, On supervised learning from sequential data with applications for speech recognition, Ph.D. thesis, Nara Institute of Science and Technology, 1999.\\\\n\\\\n[25] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple way to prevent neu- ral networks from overﬁtting.,” Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\\\\n\\\\n[26] D. Krueger, T. Maharaj, J. Kram´ar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, H. Larochelle, A. Courville, et al., “Zoneout: Regularizing RNNs by randomly preserving hidden activations,” in Proc. ICLR, 2017.\\\\n\\\\n[10] S. ¨O. Arik, G. F. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping, J. Raiman, and Y. Zhou, “Deep voice 2: Multi-speaker neural text-to-speech,” CoRR, vol. abs/1705.08947, 2017. [11] W. Ping, K. Peng, A. Gibiansky, S. ¨O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, “Deep voice 3: 2000- speaker neural text-to-speech,” CoRR, vol. abs/1710.07654, 2017.\\\\n\\\\n[12] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron: Towards end-to-end speech synthesis,” in Proc. Interspeech, Aug. 2017, pp. 4006–4010.\\\\n\\\\n[27] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pix- elCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modiﬁcations,” in Proc. ICLR, 2017.\\\\n\\\\n[28] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo, F. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman, E. Elsen, N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Wal- ters, D. Belov, and D. Hassabis, “Parallel WaveNet: Fast High- Fidelity Speech Synthesis,” CoRR, vol. abs/1711.10433, Nov. 2017.\\\\n\\\\n[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic\\\\n\\\\noptimization,” in Proc. ICLR, 2015.\\\\n\\\\n[13] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks.,” in Proc. NIPS, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds., 2014, pp. 3104–3112.\\\\n\\\\n[14] D. W. Grifﬁn and J. S. Lim, “Signal estimation from modiﬁed short-time Fourier transform,” IEEE Transactions on Acoustics, Speech and Signal Processing, pp. 236–243, 1984.\\\\n\\\\n[30] X. Gonzalvo, S. Tazari, C.-a. Chan, M. Becker, A. Gutkin, and H. Silen, “Recent advances in Google real-time HMM-driven unit selection synthesizer,” in Proc. Interspeech, 2016.\\\\n\\\\n[31] H. Zen, Y. Agiomyrgiannakis, N. Egberts, F. Henderson, and P. Szczepaniak, “Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile de- vices,” in Proc. Interspeech, 2016.\\\\n\\\\n[15] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda, “Speaker-dependent WaveNet vocoder,” in Proc. Interspeech, 2017, pp. 1118–1122.\\\\n\\\\n[16] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. Courville, and Y. Bengio, “Char2Wav: End-to-end speech synthesis,” in Proc. ICLR, 2017.', metadata={'source': 'C:\\\\\\\\Users\\\\\\\\jeeho\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\tmphlhs7xsr\\\\\\\\tmp.pdf'})]\")]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result[:3] # 논문의 제목이 먼저 뜨지 않는 것은 pdf파일의 문제이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
