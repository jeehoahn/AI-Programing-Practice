{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 색인 & 검색\n",
    "---\n",
    "\n",
    "## 코드 설명\n",
    "1. 온라인으로 사이트에서 12개의 논문 내용을 리스트 안에 넣기\n",
    "2. CharacterTextSplitter로 데이터 전처리\n",
    "3. ChromaDB로 내용을 저장\n",
    "4. Vocoder랑 관련된 논문 검색\n",
    "<br>\n",
    "\n",
    "## 어려웠던 부분들\n",
    "1. 논문을 긁어오는 과정에서 이중리스트로 만들어버려가지고 코드가 안 돌아가 당황\n",
    "2. 맨 마지막 셀에 검색을 제대로 해서 알맞은 논문을 보여주지만 한 개만 여러개 보여주는데 논문 내용을 리스트 안에 넣는 과정에서 문제가 있는 것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain chromadb openai pypdf unstructured[pdf] gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.document_loaders import OnlinePDFLoader # Unstructured[pdf]가 필요한 이유\n",
    "\n",
    "with open('PDF LINKS.txt.', 'r') as file:\n",
    "    link_numbers=file.read()\n",
    "\n",
    "link_numbers=list(link_numbers.split('\\n'))\n",
    "count=0\n",
    "for i in link_numbers:\n",
    "    link_numbers[count]=i[22:]\n",
    "    count+=1\n",
    "\n",
    "data=[]\n",
    "\n",
    "for i in link_numbers:\n",
    "    print(i)                                                   # 오래 걸리니까 잘 돌아가는지 확인하기 위해\n",
    "    loader = OnlinePDFLoader(f\"https://arxiv.org/pdf/{i}.pdf\") # 온라인 논문 가져오기\n",
    "    data.append((loader.load()[0]))                            # [0]을 붙이지 않는다면 data가 이중 리스트가 돼 다음 셀의 코드가 돌아가지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "PDF_Text=text_splitter.split_documents(data)\n",
    "\n",
    "embeddings=GPT4AllEmbeddings()\n",
    "Things = Chroma.from_documents(PDF_Text, embedding=embeddings, persist_directory=\".\")\n",
    "Things.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'})]\n",
      "[Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'})]\n",
      "[Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'})]\n",
      "[Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='generator and discriminator with auxiliary tasks. The total loss for training the vocoder model thus becomes:', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmpa6pnms_q\\\\tmp.pdf'}), Document(page_content='3. EXPERIMENTS\\n\\n3.1. Experimental Setting\\n\\n3.1.3. Evaluation metrics\\n\\nIn this section, we describe the details of our experimental settings including the dataset, model choice, hyperparameters and evaluation metrics.\\n\\n3.1.1. Dataset\\n\\nIn order to have a fair comparison with other vocoder models, we train the model on the LJSpeech dataset [29] which is also', metadata={'source': 'C:\\\\Users\\\\jeeho\\\\AppData\\\\Local\\\\Temp\\\\tmp1bc5zgbk\\\\tmp.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Vocoder\"\n",
    "Result = Things.similarity_search(query)\n",
    "\n",
    "for i in Result:\n",
    "    print(Result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
